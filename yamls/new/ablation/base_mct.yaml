name: mmbert-abl
image: mosaicml/composer:0.24.1
compute:
  cluster: r15z1p1
  gpus: 8
scheduling:
  priority: lowest
  preemptible: true
integrations:
  - integration_type: git_repo
    git_repo: JHU-CLSP/ModernBERT
    git_branch: mct
  - integration_type: pip_packages
    packages:
      - numba
      - torch-optimi
      - omegaconf
      - wandb
  - integration_type: apt_packages
    packages:
      - vim
parameters:
  # /mnt/config/parameters.yaml
  variables:
    tokenizer_name: google/gemma-2-9b
    data_local: c4-36b-baseline
    data_remote: # If blank, files must be present in data_local
    max_seq_len: 1024
    tokenizer_name: google/gemma-2-9b
    lr: 8e-4
    t_warmup: 3_000_000_000tok # Warmup to the full LR for 6% of the training duration
    bs_warmup: 50_000_000_000tok
    mlm_probability: 0.3 # FlexBERT should use 30% masking for optimal performance
    count_padding_tokens: false
    device_train_microbatch_size: 36
  # end variables
  # Model
  model:
    name: flex_bert
    pretrained_model_name: bert-base-uncased
    tokenizer_name: ${variables.tokenizer_name}
    disable_train_metrics: true
    # FlexBERT 'base' generally uses the default architecture values from the Hugging Face BertConfig object
    # Note: if using the pretrained_checkpoint argument to create a model from an existing checkpoint, make sure
    # the model_config settings match the architecture of the existing model
    model_config:
      vocab_size: 256000
      init_method: full_megatron
      num_hidden_layers: 22
      hidden_size: 768
      intermediate_size: 1152
      num_attention_heads: 12 # to have head size of 64
      attention_layer: rope
      attention_probs_dropout_prob: 0.0
      attn_out_bias: false
      attn_out_dropout_prob: 0.1
      attn_qkv_bias: false
      bert_layer: prenorm
      embed_dropout_prob: 0.0
      embed_norm: true
      final_norm: true
      skip_first_prenorm: true
      embedding_layer: sans_pos
      loss_function: fa_cross_entropy
      loss_kwargs:
        reduction: mean
      mlp_dropout_prob: 0.0
      mlp_in_bias: false
      mlp_layer: glu
      mlp_out_bias: false
      normalization: layernorm
      norm_kwargs:
        eps: 1e-5
        bias: false
      hidden_act: gelu
      head_pred_act: gelu
      activation_function: gelu # better safe than sorry
      padding: unpadded
      rotary_emb_dim: null
      rotary_emb_base: 10000.0
      local_attn_rotary_emb_base: 10000.0
      rotary_emb_scale_base: null
      rotary_emb_interleaved: false
      allow_embedding_resizing: true
      sliding_window: 128
      global_attn_every_n_layers: 3
      unpad_embeddings: true
      compile_model: true
      masked_prediction: true

  # Dataloaders
  train_loader:
    name: text
    dataset:
      local: ${variables.data_local}
      remote: ${variables.data_remote}
      split: train
      tokenizer_name: ${variables.tokenizer_name}
      max_seq_len: ${variables.max_seq_len}
      shuffle: true
      mlm_probability: ${variables.mlm_probability}
      streaming: false
      shuffle_seed: 21
    drop_last: true
    num_workers: 6
    mask_token_id: 4
    pad_token_id: 0
    bos_token_id: 2
    eos_token_id: 1
    sep_token_id: 1
    sequence_packing: true
    batch_size_warmup_min_size: ${variables.device_train_microbatch_size}
    batch_size_warmup_tokens: ${variables.bs_warmup}

  # Optimization
  scheduler:
    name: warmup_stable_decay
    t_warmup: ${variables.t_warmup}
    alpha_f: 0.02 # Linearly decay to 0.02x the full LR by the end of the training duration
    t_decay: 0tok

  optimizer:
    name: decoupled_stableadamw
    lr: ${variables.lr}
    betas:
    - 0.9
    - 0.98
    eps: 1.0e-06
    weight_decay: 1.0e-5 # Amount of weight decay regularization
    filter_bias_norm_wd: true # If True, doesn't apply weight decay to norm layers and biases
    log_grad_norm: true

  max_duration: 75_000_000_000tok
  eval_interval: 99999999999999999999999999999999ba # don't do this
  global_train_batch_size: 4608
  global_eval_batch_size: 1024

  # System
  seed: 21
  device_eval_batch_size: 36

  precision: amp_bf16

  # Logging
  progress_bar: false
  log_to_console: true
  console_log_interval: 100ba

  callbacks:
    speed_monitor:
      window_size: 100
    lr_monitor: {}
    scheduled_gc: {}
    log_grad_norm:
      batch_log_interval: 100
    packing_efficiency:
      log_interval: 100
    #hf_sync:
      #hf_repo: ${varrepo_id}
      #save_folder: ${save_folder}
      #repo_can_exist: true

  # W&B logging
  loggers:
    wandb:
      project: modernBERTv2
      entity: mmarone-jhu

  # Checkpoint to local filesystem or remote object store
  save_interval: 1_000_000tok
  save_num_checkpoints_to_keep: 100000  # Important, this cleans up checkpoints saved to DISK
  save_folder: base_pretraining/
  #repo_id: blab-jhu/${variables/run_name}

  # Load from local filesystem or remote object store to
  # load_path: null

  autoresume: true


command: |-
  # blab-jhu/sample-multi
  set -x
  cd /ModernBERT
  composer download.py --pattern "*" --repo orionweller/c4_very_small_sample --output c4-36b-baseline/ --workers 40 --dist
  #ls c4-36b-baseline/
  #composer main.py /mnt/config/parameters.yaml
  sleep infinity
